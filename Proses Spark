from pyspark.context import SparkContext, SparkConf

# Initialize SparkContext
conf = SparkConf().setAppName("IoTLightDetection").setMaster("local[*]")
sc = SparkContext(conf=conf)

# Load the CSV file
csv_file_path = "file:///home/kathleenevel/iot_telemetry_data.csv"
data_rdd = sc.textFile(csv_file_path)

# Extract the header and filter it out
header = data_rdd.first()
data_rdd = data_rdd.filter(lambda row: row != header)

# Split each row by commas and parse fields
data_rdd = data_rdd.map(lambda row: row.split(","))

# Filter rows where light detection is True
data_filtered_rdd = data_rdd.filter(lambda row: row[4].strip().lower() == "true")

# Map to (device, (temp, humidity, 1))
# temp = float(row[8]), humidity = float(row[3])
data_mapped_rdd = data_filtered_rdd.map(lambda row: (
    row[1],  # device
    (float(row[8]), float(row[3]), 1)  # (temp, humidity, count)
))

# Reduce by key to sum up temp, humidity, and count
data_reduced_rdd = data_mapped_rdd.reduceByKey(lambda a, b: (
    a[0] + b[0],  # Sum of temp
    a[1] + b[1],  # Sum of humidity
    a[2] + b[2]   # Count
))

# Calculate averages for each device
data_avg_rdd = data_reduced_rdd.mapValues(lambda x: (
    x[0] / x[2],  # Average temp
    x[1] / x[2]   # Average humidity
))

# Collect the results and display them
results = data_avg_rdd.collect()
for device, (avg_temp, avg_humidity) in results:
    print(f"Device: {device}, Average Temperature: {avg_temp:.2f}, Average Humidity: {avg_humidity:.2f}")

# Stop the SparkContext
sc.stop()
